{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/philosophy-question-answerer/model-tests-automated/blob/main/model_test_results_scoring.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations, Imports and Third-Party Services"
      ],
      "metadata": {
        "id": "BqBxBkUrM5B4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4M8rmj65ygBr",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "! pip install cohere\n",
        "! pip install ratelimit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import pprint\n",
        "import cohere\n",
        "from collections import OrderedDict\n",
        "from ratelimit import limits\n",
        "from google.colab import userdata, drive"
      ],
      "metadata": {
        "id": "QgACtJG6y4j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COHERE_API_KEY = userdata.get('COHERE_API_KEY')\n",
        "# COHERE_API_KEY = userdata.get('COHERE_API_KEY_2')\n",
        "# COHERE_API_KEY = userdata.get('COHERE_API_KEY_3')"
      ],
      "metadata": {
        "id": "oEqekLcS0jJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "test_results_dir = '/content/drive/My Drive/Model Tests Results Cleaned'"
      ],
      "metadata": {
        "id": "p795VH8aMXfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "ANxh0RKUMhb5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract All QA Pairs From All Test Result Files on Google Drive"
      ],
      "metadata": {
        "id": "J75ZJuKMMlgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_combinations = {}\n",
        "\n",
        "qa_pattern = re.compile(r\"QUESTION:\\s*(.*?)\\s*ANSWER:\\s*(.*?)\\s*INFERENCE_TIME:\\s*([\\d.]+)( seconds)?\", re.DOTALL)\n",
        "\n",
        "for combination in os.listdir(test_results_dir):\n",
        "\n",
        "    print(f'Processing file {combination}')\n",
        "\n",
        "    combination_path = os.path.join(test_results_dir, combination)\n",
        "\n",
        "    with open(combination_path, 'r') as file:\n",
        "        file_content = file.read()\n",
        "\n",
        "        question_data = []\n",
        "\n",
        "        matches = qa_pattern.findall(file_content)\n",
        "\n",
        "        for match in matches:\n",
        "            question_data.append({\n",
        "                'question': match[0].strip(),\n",
        "                'answer': match[1].strip(),\n",
        "                'inference_time': int(match[2]),\n",
        "                'score': None\n",
        "            })\n",
        "\n",
        "        combination_key = os.path.splitext(combination)[0]\n",
        "        all_combinations[combination_key] = question_data"
      ],
      "metadata": {
        "id": "dP8CVDIgGKZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print Resulting Dictionary"
      ],
      "metadata": {
        "id": "sCpAUmTANM_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pprint.pprint(all_combinations)"
      ],
      "metadata": {
        "id": "7k2LnLJ7JfMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cohere Functions"
      ],
      "metadata": {
        "id": "Kh0czL2hNSWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Prompt"
      ],
      "metadata": {
        "id": "x9xVLvkJKMqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(question, answer):\n",
        "  return f'''\n",
        "  Consider the following question about Ludwig Wittgenstein's Philosophical Investigations that a philosophy student may ask his/her professor: \"{question}\"\\n\n",
        "  The following is a candidate answer to the given question provided by an AI model in training: \"{answer}\"\\n\n",
        "  Evaluate this answer based on its accuracy, thoroughness, coherency and relevancy using your own knowledge of Wittgenstein's Philosophical Investigations, and strictly return ONLY an integer score out of 100.\n",
        "  '''"
      ],
      "metadata": {
        "id": "iSfD0heyKMYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query Cohere"
      ],
      "metadata": {
        "id": "4VRuJYWzKIrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "co = cohere.Client(COHERE_API_KEY)\n",
        "\n",
        "one_minute = 60\n",
        "\n",
        "@limits(calls=99, period=one_minute)\n",
        "def query_cohere(prompt):\n",
        "    response = co.chat(message=prompt, model='command', temperature=0.9)\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "MzDEmBMdJva1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parse Score From Cohere Response"
      ],
      "metadata": {
        "id": "Q_WHhtzHdxNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_response(response):\n",
        "\n",
        "    pattern = re.compile(r'\\b([1-9]|[1-9][0-9]|100)\\b')\n",
        "\n",
        "    match = re.search(pattern, response)\n",
        "\n",
        "    if match:\n",
        "        return int(match.group())\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "pxJy1xEBceck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Score All Model Combinations"
      ],
      "metadata": {
        "id": "tYz8eqIZO_Ww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collect Cohere Responses"
      ],
      "metadata": {
        "id": "et2nP-n8cFXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for combination in all_combinations:\n",
        "\n",
        "  print(f'Processing combination {combination}')\n",
        "\n",
        "  for qa_pair in all_combinations[combination]:\n",
        "\n",
        "    question = qa_pair['question']\n",
        "    answer = qa_pair['answer']\n",
        "    inference_time = qa_pair['inference_time']\n",
        "\n",
        "    prompt = generate_prompt(question, answer)\n",
        "\n",
        "    try:\n",
        "      response = query_cohere(prompt)\n",
        "    except ratelimit.RateLimitException:\n",
        "      print(\"Rate limit exceeded.\")\n",
        "      time.sleep(60)\n",
        "      response = query_cohere(prompt)\n",
        "\n",
        "    score = parse_response(response)\n",
        "\n",
        "    qa_pair['score'] = score"
      ],
      "metadata": {
        "id": "rkWYOYVDQ-bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint.pprint(all_combinations)"
      ],
      "metadata": {
        "id": "3pe-sIVRiVBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Average Score & Ratio of Scored Questions for Each Combination"
      ],
      "metadata": {
        "id": "hqP_r7TPsgu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_combinations_avg_scores = {}\n",
        "\n",
        "for combination in all_combinations:\n",
        "\n",
        "  combination_avg_score = 0\n",
        "  non_none_count = 0\n",
        "  num_qa_pairs = len(all_combinations[combination])\n",
        "\n",
        "  for qa_pair in all_combinations[combination]:\n",
        "\n",
        "        print(qa_pair['score'])\n",
        "        if qa_pair['score'] is None:\n",
        "            continue\n",
        "\n",
        "        combination_avg_score += qa_pair['score']\n",
        "        non_none_count += 1\n",
        "\n",
        "  if non_none_count > 0:\n",
        "    combination_avg_score /= non_none_count\n",
        "  else:\n",
        "    combination_avg_score = None\n",
        "\n",
        "  combination_avg_score = round(combination_avg_score, 1)\n",
        "\n",
        "  ratio_scored_qa_pairs = non_none_count / num_qa_pairs * 100\n",
        "  ratio_str = str(round(ratio_scored_qa_pairs, 1)) + \"%\"\n",
        "\n",
        "  all_combinations_avg_scores[combination] = {\n",
        "        'avg_score': combination_avg_score,\n",
        "        'ratio_scored_qa_pairs': ratio_str\n",
        "    }\n",
        "\n",
        "print(all_combinations_avg_scores)"
      ],
      "metadata": {
        "id": "fLW2WEKMsgFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sort Combinations by Score\n"
      ],
      "metadata": {
        "id": "CUYQ4P6p2JF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_models = OrderedDict(sorted(all_combinations_avg_scores.items(), key=lambda x: x[1]['avg_score'], reverse=True))\n",
        "\n",
        "pprint.pprint(sorted_models)"
      ],
      "metadata": {
        "id": "qtqJPT932NGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Results to a Text File"
      ],
      "metadata": {
        "id": "aqV17zzoGoR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = '/content/drive/My Drive'\n",
        "file_name = 'combinations_sorted_by_avg_score'\n",
        "\n",
        "file_path = os.path.join(save_dir, file_name)\n",
        "\n",
        "with open(file_path, 'w') as file:\n",
        "    for combination, stats in sorted_models.items():\n",
        "        file.write(combination + '\\n')\n",
        "        file.write(f\"avg_score: {stats['avg_score']}\\n\")\n",
        "        file.write(f\"ratio_scored_qa_pairs: {stats['ratio_scored_qa_pairs']}\\n\")\n",
        "        file.write('\\n\\n')"
      ],
      "metadata": {
        "id": "gAtZ7OmHGqPv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}