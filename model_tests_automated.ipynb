{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/philosophy-question-answerer/model-tests-automated/blob/main/model_tests_automated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "fff2112dd7a54f95943d85f3b7567f9b",
        "deepnote_cell_type": "text-cell-h1",
        "id": "fQ3fbfhCxDoS"
      },
      "source": [
        "# Pip Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "b46f4a21755b46609a284cd0b7a4dedd",
        "deepnote_cell_type": "code",
        "id": "jwl7_upUxDoT"
      },
      "source": [
        "! pip install cohere\n",
        "! pip install ctransformers\n",
        "! pip install huggingface_hub\n",
        "! pip install langchain\n",
        "! pip install pinecone-client\n",
        "! pip install sentence-transformers\n",
        "! pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "c5ea336b5ac34c28b475f7c9d8295a9e",
        "deepnote_cell_type": "text-cell-h1",
        "id": "ngIxudooxDoU"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "2eace5b639774f8fb67e845362b77cbd",
        "deepnote_cell_type": "code",
        "id": "lAMlj-NFxDoU"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import uuid\n",
        "\n",
        "import torch.cuda\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from ctransformers import AutoModelForCausalLM\n",
        "import cohere\n",
        "\n",
        "import pinecone\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "from google.colab import userdata, drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up Google Drive"
      ],
      "metadata": {
        "id": "qBUayTW_Y4gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "!mkdir -p \"/content/drive/My Drive/Model Tests Results\""
      ],
      "metadata": {
        "id": "nzlZ7NVIY9VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "76604d8902b64de6a264a21670f1c7f3",
        "deepnote_cell_type": "text-cell-h1",
        "id": "fnjVWk7cxDoU"
      },
      "source": [
        "# Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "ce69b3c24e8346b4bddf4227e36087a2",
        "deepnote_cell_type": "code",
        "id": "PIOtGRAnxDoV"
      },
      "source": [
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "PINECONE_ENVIRONMENT = userdata.get('PINECONE_ENVIRONMENT')\n",
        "COHERE_API_KEY = userdata.get('COHERE_API_KEY')\n",
        "PINECONE_INDEX_NAME = 'test-aristotle'\n",
        "VECTOR_DIMENSION = 384 # sentence-transformers/all-MiniLM-L6-v2\n",
        "TEST_QUESTIONS = [\n",
        "    'What determines the meaning of a word?',\n",
        "    'When can something can be classified as a game?',\n",
        "    'What is the concept of family resemblance?',\n",
        "    'Is the existence of a private language possible?',\n",
        "    'What does following a rule entail?',\n",
        "    'Can the concept of sameness be used to teach a rule?',\n",
        "    'What is the role of language-games?',\n",
        "    'How is the idea that mental processes form the basis of our understanding of language critiqued?',\n",
        "    'Do private mental objects exist?',\n",
        "    'What is the relationship between forms of life and language?',\n",
        "    'How is it explained that the meaning of a word is its use in language?',\n",
        "    'How do philosophical problems arise from misunderstandings of language?',\n",
        "    'What is problematic about the Augustinian view on meaning?',\n",
        "    'What is said about the misguided nature of philosophical questions?',\n",
        "    'What is the purpose of the analogy with the toolbox?',\n",
        "    'In what ways does the notion of \\'language-games\\' challenge traditional epistemology?',\n",
        "    'What implications does the critique of private language have for theories of consciousness?',\n",
        "    'How is the concept of pain used to argue against the possibility of a private language?',\n",
        "    'In what ways does the examination of pain\\'s sameness challenge the understanding of subjective experiences?',\n",
        "    'What are the limitations of language?',\n",
        "    'What is the relationship between forms of life and understanding philosophy?'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "0fe3cd236e464ca7a20ae937624be8d5",
        "deepnote_cell_type": "text-cell-h1",
        "id": "zNIXPz1dxDoV"
      },
      "source": [
        "# Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "e26da970a13e4753b705c3e7a8657193",
        "deepnote_cell_type": "code",
        "id": "PnejRPMkxDoV"
      },
      "source": [
        "dataset_path = './dataset'\n",
        "\n",
        "files = os.listdir(dataset_path)\n",
        "\n",
        "documents = []\n",
        "\n",
        "for file in files:\n",
        "    if file.endswith('.txt'):\n",
        "        file_path = os.path.join(dataset_path, file)\n",
        "\n",
        "        loader = TextLoader(file_path, encoding='UTF-8')\n",
        "        document = loader.load()\n",
        "\n",
        "        documents.append(document)\n",
        "\n",
        "print(f'Loaded {len(documents)} documents.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "ae761eb1f6eb4e5aa3b22f15d8ad2fb0",
        "deepnote_cell_type": "text-cell-h1",
        "id": "HgeKdzFjxDoW"
      },
      "source": [
        "# Utility Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "dc600a8894224e33b40d735dd6cd67af",
        "deepnote_cell_type": "text-cell-h2",
        "id": "-i1dbuE7xDoW"
      },
      "source": [
        "## Create Chunks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "fe18edbe16a04fe7afbbc3899751e21c",
        "deepnote_cell_type": "code",
        "id": "5D_RJ9SWxDoW"
      },
      "source": [
        "def create_chunks(documents, chunk_size=750, chunk_overlap=50):\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
        "                                                   chunk_overlap=chunk_overlap,\n",
        "                                                   separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"])\n",
        "    all_chunks = []\n",
        "\n",
        "    for document in documents:\n",
        "        chunks = text_splitter.split_documents(document)\n",
        "        all_chunks.extend(chunks)\n",
        "\n",
        "    return all_chunks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "5e2cfe4e3aa34a738a51f0883d2f6ecb",
        "deepnote_cell_type": "text-cell-h2",
        "id": "bCSkc0YxxDoX"
      },
      "source": [
        "## Load Model Locally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": null,
        "output_cleared": true,
        "execution_start": 1707656539306,
        "execution_millis": 9,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "cc5e08f7daa7407b8a18a91346e75716",
        "deepnote_cell_type": "code",
        "id": "ogZ85Nt9xDoX"
      },
      "source": [
        "models_dict =\n",
        "    { 'Mistral': {\n",
        "                    'pretrained_model_name_or_path': 'TheBloke/Mistral-7B-Instruct-v0.1-GGUF',\n",
        "                    'model_file': 'mistral-7b-instruct-v0.1.Q4_K_M.gguf',\n",
        "                    'model_type': 'mistral',\n",
        "                }\n",
        "\n",
        "    }\n",
        "\n",
        "def load_model_locally(model_choice):\n",
        "\n",
        "    return AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_choice['pretrained_model_name_or_path'],\n",
        "                                            model_file=model_choice['model_file'],\n",
        "                                            model_type=model_choice['model_type'],\n",
        "                                            gpu_layers=0\n",
        "                                            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "606e660aa2054bf9a2c15b38bc3bfd8d",
        "deepnote_cell_type": "text-cell-h2",
        "id": "-ffKCP93xDoY"
      },
      "source": [
        "## Query Cohere Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "ea439e106f934105b755a1bc97fedf86",
        "deepnote_cell_type": "code",
        "id": "8PJB133PxDoY"
      },
      "source": [
        "co = cohere.Client(COHERE_API_KEY)\n",
        "\n",
        "def query_cohere(prompt):\n",
        "    response = co.chat(message=prompt, model=\"command\", temperature=0.9)\n",
        "    return response.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "39d6cd8eb7f440d19b98ef976f72f268",
        "deepnote_cell_type": "text-cell-h2",
        "id": "5OoL2xkExDoY"
      },
      "source": [
        "## Save Results to .txt File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "4d041d1c9fda4f3e9ce23e20659e9844",
        "deepnote_cell_type": "code",
        "id": "Ah_3PvGbxDoY"
      },
      "source": [
        "def save_results_to_txt(result_string, model_name, chunk_size, chunk_overlap, is_reranking):\n",
        "\n",
        "  results_dir = '/content/drive/My Drive/Model Tests Results'\n",
        "\n",
        "  output_file_path = f'{results_dir}/{model_name}_{chunk_size}_{chunk_overlap}_{is_reranking}.txt'\n",
        "\n",
        "  with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "\n",
        "    file.write(f'\\nModel = {model_name}\\n')\n",
        "    file.write(f'Chunk Size = {chunk_size}\\n')\n",
        "    file.write(f'Chunk Overlap = {chunk_overlap}\\n')\n",
        "    file.write(f'Reranking on = {is_reranking}\\n\\n')\n",
        "\n",
        "    file.write(f'{result_string}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "ca133b4a6d10450a9e8ab2667c45f087",
        "deepnote_cell_type": "text-cell-h2",
        "id": "-_oU8zbmxDoZ"
      },
      "source": [
        "## Format Question-Context-Answer (QCA) String"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": null,
        "execution_start": 1707659412078,
        "execution_millis": 7,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "d15cc8e112084028b5c058acc029ecc1",
        "deepnote_cell_type": "code",
        "id": "tiRlVenvxDoZ"
      },
      "source": [
        "def format_qca_string(new_question, new_context, new_answer, inference_time):\n",
        "    return f'''\n",
        "        QUESTION:\\n\n",
        "        {new_question}\\n\\n\n",
        "\n",
        "        CONTEXT:\\n\n",
        "        {new_context}\\n\\n\n",
        "\n",
        "        ANSWER:\\n\n",
        "        {new_answer}\\n\\n\n",
        "\n",
        "        INFERENCE TIME:\\n\n",
        "        {inference_time}\\n\\n\n",
        "\n",
        "        ####################### \\n\\n\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "58b51c4d0b9642e48abbdf91feb5ede9",
        "deepnote_cell_type": "text-cell-h1",
        "id": "L79BCh1OxDoZ"
      },
      "source": [
        "# Creating Vector Database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "661a69455d5846a597943a818fcc8df3",
        "deepnote_cell_type": "code",
        "id": "dQ-F1lBQxDoZ"
      },
      "source": [
        "pc = pineconePinecone(api_key=PINECONE_API_KEY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "7ffa002b5618435a913456adfeb346f0",
        "deepnote_cell_type": "text-cell-h2",
        "id": "IuDlRUZjxDoZ"
      },
      "source": [
        "## Load the Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "68f1ca91ebdd4092b5499a32477f79f2",
        "deepnote_cell_type": "code",
        "id": "YQ6dpDCmxDoZ"
      },
      "source": [
        "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "embed_model = HuggingFaceEmbeddings(\n",
        "    model_name=embed_model_id,\n",
        "    model_kwargs={'device': device},\n",
        "    encode_kwargs={'device': device, 'batch_size': 10}\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "3520a76becee455bab9eb38c5095b704",
        "deepnote_cell_type": "text-cell-h2",
        "id": "5xNr_dydxDoZ"
      },
      "source": [
        "## Create and Populate the Database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "eaaf97757ae74d5fbdb4355c676112b3",
        "deepnote_cell_type": "code",
        "id": "gpbiS6eWxDoZ"
      },
      "source": [
        "def create_and_populate_db(chunks, vector_dimension):\n",
        "\n",
        "    spec = pinecone.PodSpec(environment=PINECONE_ENVIRONMENT)\n",
        "\n",
        "    if any(index.name == PINECONE_INDEX_NAME for index in pc.list_indexes()):\n",
        "    pc.delete_index(PINECONE_INDEX_NAME)\n",
        "\n",
        "    pc.create_index(\n",
        "        name=PINECONE_INDEX_NAME,\n",
        "        dimension=vector_dimension,\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "        )\n",
        "\n",
        "    while not pc.describe_index(PINECONE_INDEX_NAME).status['ready']:\n",
        "    time.sleep(1)\n",
        "\n",
        "    index = pc.Index(PINECONE_INDEX_NAME)\n",
        "    index.describe_index_stats()\n",
        "\n",
        "    batch_size = 10\n",
        "\n",
        "    for i in tqdm(range(0, len(all_chunks), batch_size)):\n",
        "        i_end = min(len(all_chunks), i+batch_size)\n",
        "        batch = all_chunks[i:i_end]\n",
        "        texts = [chunk.page_content for chunk in batch]\n",
        "        ids = [ str(uuid.uuid4()) ] * len(batch)\n",
        "        embeds = embed_model.embed_documents(texts)\n",
        "        metadata = [\n",
        "            {\n",
        "                'text': chunk.page_content,\n",
        "                'source': chunk.metadata['source']\n",
        "            } for chunk in batch\n",
        "        ]\n",
        "        index.upsert(vectors=zip(ids, embeds, metadata))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "aa944a5eca604cb288e49ae3bef102e2",
        "deepnote_cell_type": "text-cell-h2",
        "id": "kY5lSsKBxDoa"
      },
      "source": [
        "## Querying the Database (Semantic Search)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "0092dbd02ba645da94b5dcc3028ec92a",
        "deepnote_cell_type": "code",
        "id": "ybsa1qetxDoa"
      },
      "source": [
        "def semantic_search(question, num_matched_excerpts):\n",
        "\n",
        "    index = pc.Index(PINECONE_INDEX_NAME)\n",
        "    question_embedding = embed_model.embed_query(question)\n",
        "    res = index.query(vector=question_embedding, top_k=num_matched_excerpts, include_metadata=True)\n",
        "\n",
        "    docs = [\n",
        "        {\n",
        "          'source': result['metadata']['source'],\n",
        "          'text': result['metadata']['text']\n",
        "        } for result in res['matches']\n",
        "    ]\n",
        "\n",
        "    return docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "0ebd2f4f872d4a86aa2ab5d26d02988a",
        "deepnote_cell_type": "text-cell-h1",
        "id": "qUdOmftmxDoa"
      },
      "source": [
        "# Reranking (Cohere API)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "9ce18b409e634652bd8dd943faadb1ac",
        "deepnote_cell_type": "code",
        "id": "YqCUOjQMxDoa"
      },
      "source": [
        "def semantic_search_reranking(question, num_matched_excerpts, num_reranked_excerpts):\n",
        "    docs = get_docs(query=question, top_k=num_matched_excerpts)\n",
        "    text_to_source = { doc['text'] : doc['source'] for doc in docs }\n",
        "    reranked_docs = co.rerank(query=question,\n",
        "                             documents=[ doc['text'] for doc in docs ],\n",
        "                             top_n=num_reranked_excerpts,\n",
        "                             model='rerank-english-v2.0')\n",
        "    reranked_texts = [ doc.document['text'] for doc in reranked_docs ]\n",
        "\n",
        "    docs = [\n",
        "        {\n",
        "          'text': text,\n",
        "          'source': text_to_source[text]\n",
        "        } for text in reranked_texts\n",
        "    ]\n",
        "\n",
        "    return docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "7f13d6ee25084da8abbe52aef0dfcec5",
        "deepnote_cell_type": "text-cell-h1",
        "id": "lS01qGbZxDoa"
      },
      "source": [
        "# Retrieval Augmented Generation (RAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "def12808e72e45f0a84c77bd0def1f24",
        "deepnote_cell_type": "text-cell-h2",
        "id": "SEuB6Ny6xDoa"
      },
      "source": [
        "## Create the Question-Context Prompt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "87f85012778340ceb59c78ee7f74b922",
        "deepnote_cell_type": "code",
        "id": "XnP94hfQxDoa"
      },
      "source": [
        "def extract_context(question, num_matched_excerpts, is_reranking, num_reranked_excerpts):\n",
        "\n",
        "    if is_reranking:\n",
        "        docs = semantic_search_reranking(question=question,\n",
        "                                         num_matched_excerpts=num_matched_excerpts,\n",
        "                                         num_reranked_excerpts=num_reranked_excerpts\n",
        "                                        )\n",
        "    else:\n",
        "        docs = semantic_search(question=question, num_matched_excerpts=num_matched_excerpts)\n",
        "\n",
        "    context = ''\n",
        "    for doc in docs:\n",
        "        context = context + textwrap.fill(doc['text'], 150) + '\\n' + textwrap.fill(doc['source'], 150) + '\\n\\n'\n",
        "\n",
        "    return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "a9de4985f7c44b048510d2a12662adbe",
        "deepnote_cell_type": "code",
        "id": "oBdg-7WyxDob"
      },
      "source": [
        "def create_prompt(question, context):\n",
        "\n",
        "  prompt = f'''\n",
        "  Answer the following QUESTION with the given CONTEXT. \\n\\n\n",
        "  QUESTION: {question} \\n\n",
        "  CONTEXT: \\n {context} \\n\n",
        "  ANSWER:\n",
        "  '''\n",
        "\n",
        "  return prompt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "55cb36f0a2ac4668b5f46048913e22dd",
        "deepnote_cell_type": "text-cell-h2",
        "id": "ODSaPrWjxDob"
      },
      "source": [
        "## Run Tests - Vary Model, Vector DB Parameters, and Reranking On/Off"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "dcf0831b9d1a46d5bf3f76b345080fd2",
        "deepnote_cell_type": "code",
        "id": "SfAThZKJxDob"
      },
      "source": [
        "chunk_parameter_combinations = [\n",
        "    {'chunk_size': 256, 'chunk_overlap': 25},\n",
        "    {'chunk_size': 256, 'chunk_overlap': 50},\n",
        "    {'chunk_size': 512, 'chunk_overlap': 50},\n",
        "    {'chunk_size': 512, 'chunk_overlap': 100},\n",
        "    {'chunk_size': 1024, 'chunk_overlap': 100},\n",
        "    {'chunk_size': 1024, 'chunk_overlap': 150},\n",
        "    {'chunk_size': 1500, 'chunk_overlap': 100},\n",
        "    {'chunk_size': 1500, 'chunk_overlap': 300},\n",
        "    {'chunk_size': 2048, 'chunk_overlap': 200},\n",
        "    {'chunk_size': 2048, 'chunk_overlap': 350}\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "151f77df93db4ce4a05c03b2f70a3316",
        "deepnote_cell_type": "code",
        "id": "aPiLDDb-xDob"
      },
      "source": [
        "mistral = load_model_locally(models_dict['Mistral'])\n",
        "# falcon = load_model_locally(models_dict['Falcon'])\n",
        "\n",
        "for combination in tqdm(chunk_parameter_combinations):\n",
        "\n",
        "    chunk_size = combination['chunk_size']\n",
        "    chunk_overlap = combination['chunk_overlap']\n",
        "\n",
        "    chunks = create_chunks(documents, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    create_and_populate_db(chunks=chunks, vector_dimension=VECTOR_DIMENSION)\n",
        "\n",
        "    for is_reranking in tqdm([True, False]):\n",
        "\n",
        "        combination_i_output = ''\n",
        "\n",
        "        print(f'Processing combination: Chunk Size: {chunk_size}, Chunk Overlap: {chunk_overlap}, Reranking: {is_reranking}')\n",
        "\n",
        "        for question in tqdm(TEST_QUESTIONS):\n",
        "\n",
        "            print(f'Querying question {TEST_QUESTIONS.index(question)}')\n",
        "\n",
        "            context = extract_context(question=question,\n",
        "                                      num_matched_excerpts=25,\n",
        "                                      is_reranking=is_reranking,\n",
        "                                      num_reranked_excerpts=3)\n",
        "\n",
        "            prompt = create_prompt(question=question, context=context)\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            answer = mistral(prompt)\n",
        "            # answer = query_cohere(prompt)\n",
        "\n",
        "            end_time = time.time()\n",
        "            inference_time = end_time - start_time\n",
        "\n",
        "            combination_i_output += format_qca_string(new_question=question,\n",
        "                                                     new_context=context\n",
        "                                                     new_answer=answer,\n",
        "                                                     inference_time=inference_time)\n",
        "\n",
        "        save_results_to_txt(result_string=combination_i_output,\n",
        "                            model_name='Mistral',\n",
        "                            chunk_size=chunk_size,\n",
        "                            chunk_overlap=chunk_overlap,\n",
        "                            is_reranking=is_reranking)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_full_width": true,
    "deepnote_notebook_id": "c4f7f37be5c6444e99d25711c7e1d018",
    "deepnote_execution_queue": [],
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  }
}